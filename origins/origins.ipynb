{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import shape\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full merge bash script:\n",
    "\n",
    "`gdal_merge.py -o 2017cdls-v2.tif ./2017_cdls/VT/cdl_30m_r_vt_2017_albers.tif ./2017_cdls/VA/cdl_30m_r_va_2017_albers.tif ./2017_cdls/SD/cdl_30m_r_sd_2017_albers.tif ./2017_cdls/SC/cdl_30m_r_sc_2017_albers.tif ./2017_cdls/UT/cdl_30m_r_ut_2017_albers.tif ./2017_cdls/GA/cdl_30m_r_ga_2017_albers.tif ./2017_cdls/MS/cdl_30m_r_ms_2017_albers.tif ./2017_cdls/MT/cdl_30m_r_mt_2017_albers.tif ./2017_cdls/MO/cdl_30m_r_mo_2017_albers.tif ./2017_cdls/MA/cdl_30m_r_ma_2017_albers.tif ./2017_cdls/KY/cdl_30m_r_ky_2017_albers.tif ./2017_cdls/AL/cdl_30m_r_al_2017_albers.tif ./2017_cdls/NH/cdl_30m_r_nh_2017_albers.tif ./2017_cdls/MN/cdl_30m_r_mn_2017_albers.tif ./2017_cdls/MI/cdl_30m_r_mi_2017_albers.tif ./2017_cdls/OK/cdl_30m_r_ok_2017_albers.tif ./2017_cdls/IN/cdl_30m_r_in_2017_albers.tif ./2017_cdls/CO/cdl_30m_r_co_2017_albers.tif ./2017_cdls/CA/cdl_30m_r_ca_2017_albers.tif ./2017_cdls/IA/cdl_30m_r_ia_2017_albers.tif ./2017_cdls/CT/cdl_30m_r_ct_2017_albers.tif ./2017_cdls/FL/cdl_30m_r_fl_2017_albers.tif ./2017_cdls/WV/cdl_30m_r_wv_2017_albers.tif ./2017_cdls/RI/cdl_30m_r_ri_2017_albers.tif ./2017_cdls/WY/cdl_30m_r_wy_2017_albers.tif ./2017_cdls/TX/cdl_30m_r_tx_2017_albers.tif ./2017_cdls/PA/cdl_30m_r_pa_2017_albers.tif ./2017_cdls/NC/cdl_30m_r_nc_2017_albers.tif ./2017_cdls/ND/cdl_30m_r_nd_2017_albers.tif ./2017_cdls/NM/cdl_30m_r_nm_2017_albers.tif ./2017_cdls/NJ/cdl_30m_r_nj_2017_albers.tif ./2017_cdls/ME/cdl_30m_r_me_2017_albers.tif ./2017_cdls/AR/cdl_30m_r_ar_2017_albers.tif ./2017_cdls/NV/cdl_30m_r_nv_2017_albers.tif ./2017_cdls/DC/cdl_30m_r_dc_2017_albers.tif ./2017_cdls/MD/cdl_30m_r_md_2017_albers.tif ./2017_cdls/KS/cdl_30m_r_ks_2017_albers.tif ./2017_cdls/NE/cdl_30m_r_ne_2017_albers.tif ./2017_cdls/DE/cdl_30m_r_de_2017_albers.tif ./2017_cdls/AZ/cdl_30m_r_az_2017_albers.tif ./2017_cdls/NY/cdl_30m_r_ny_2017_albers.tif ./2017_cdls/ID/cdl_30m_r_id_2017_albers.tif ./2017_cdls/OH/cdl_30m_r_oh_2017_albers.tif ./2017_cdls/OR/cdl_30m_r_or_2017_albers.tif ./2017_cdls/IL/cdl_30m_r_il_2017_albers.tif ./2017_cdls/LA/cdl_30m_r_la_2017_albers.tif ./2017_cdls/WI/cdl_30m_r_wi_2017_albers.tif ./2017_cdls/WA/cdl_30m_r_wa_2017_albers.tif ./2017_cdls/TN/cdl_30m_r_tn_2017_albers.tif -n 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read '../unsynced-data/ri.geojson' into a GeoDataFrame\n",
    "df = gpd.read_file('../unsynced-data/ri.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_col equal all columns that begin with crop\n",
    "crop_col = [c for c in df.columns if c.startswith('crop')]\n",
    "crop_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a total value column that is the sum of all crop columns\n",
    "df['total_c'] = df[crop_col].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a max_c column that is the max of all crop columns\n",
    "df['max_cval'] = df[crop_col].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a max_c column that is the colmn name thatis max of all crop columns\n",
    "df['max_c'] = df[crop_col].idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.to_file('../unsynced-data/ri.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cdl-code.csv\n",
    "cdl = pd.read_csv('../unsynced-data/cdl-codes.csv')\n",
    "cdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove crop_0 from crop_col\n",
    "crop_col.remove('crop_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column names in column names list replace the column name with the value after the first _ in the column name\n",
    "col_num = []\n",
    "for c in crop_col:\n",
    "    cropnum = c.split('_')[1]\n",
    "    # change crop column name to value in class_name where Codes is cropnum\n",
    "    df.rename(columns={c: cdl.loc[cdl['Codes'] == int(cropnum), 'Class_Names'].values[0]}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from max_c remove crop_\n",
    "df['max_c'] = df['max_c'].str.replace('crop_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace value in max_c with value in class_name where Codes is max_c\n",
    "df['max_c'] = df['max_c'].apply(lambda x: cdl.loc[cdl['Codes'] == int(x), 'Class_Names'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_crop = ['Developed', 'Clouds/No Data', 'Water', 'Developed/Open Space', 'Developed/Low Intensity', 'Developed/Med Intensity', 'Developed/High Intensity', 'Open Water', 'Perennial Ice/Snow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = gpd.read_file('../unsynced-data/ca.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.head(1000).explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_crop_names = [\"Background\", \"Developed\", \"Clouds/No Data\", 'Water', 'Developed/Open Space', 'Developed/Low Intensity', 'Developed/Med Intensity', 'Developed/High Intensity', 'Open Water', 'Perennial Ice/Snow']\n",
    "non_crop_codes = [\"crop_0\", \"crop_111\", \"crop_112\", \"crop_121\", \"crop_122\", \"crop_123\", \"crop_124\", \"crop_81\", \"crop_82\", \"crop_83\"]\n",
    "cdl = pd.read_csv('../unsynced-data/cdl-codes.csv')\n",
    "def grid_prep(df):\n",
    "    # crop_col equal all columns that begin with crop\n",
    "    code_col = [c for c in df.columns if c.startswith('crop')]\n",
    "    crop_col = [c for c in code_col if c not in non_crop_codes]\n",
    "    print (code_col)\n",
    "    print (crop_col)\n",
    "    # make a total value column that is the sum of all crop columns\n",
    "    df['total_c'] = df[crop_col].sum(axis=1)\n",
    "    #make a max_c column that is the max of all crop columns\n",
    "    df['max_cval'] = df[crop_col].max(axis=1)\n",
    "    #make a max_c column that is the colmn name thatis max of all crop columns\n",
    "    df['max_c'] = df[crop_col].idxmax(axis=1)\n",
    "    # for column names in column names list replace the column name with the value after the first _ in the column name\n",
    "    class_names = []\n",
    "    for c in code_col:\n",
    "        cropnum = c.split('_')[1]\n",
    "        # add the Class_Names value that matches cropnum to class_names list\n",
    "        class_names.append(cdl.loc[cdl['Codes'] == int(cropnum), 'Class_Names'].values[0])\n",
    "        # change crop column name to value in class_name where Codes is cropnum\n",
    "        df.rename(columns={c: cdl.loc[cdl['Codes'] == int(cropnum), 'Class_Names'].values[0]}, inplace=True)\n",
    "        # from max_c remove crop_\n",
    "    # from max_c remove crop_\n",
    "    print (class_names)\n",
    "    df['max_c'] = df['max_c'].str.replace('crop_', '')\n",
    "    df['max_c'] = df['max_c'].apply(lambda x: cdl.loc[cdl['Codes'] == int(x), 'Class_Names'].values[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_clean = grid_prep(ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to read the megafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file as gpd\n",
    "full_grid = gpd.read_file(\"../unsynced-data/grid_z15-crops.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_clean.to_file('../unsynced-data/ca.geojson', driver='GeoJSON')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to get rid of the 0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Decimal is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/camkruse/projects/earthgenome/food-system-digital-twin/origins/origins.ipynb Cell 29\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camkruse/projects/earthgenome/food-system-digital-twin/origins/origins.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# If the value is not 0, write the feature to the output GeoJSON file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/camkruse/projects/earthgenome/food-system-digital-twin/origins/origins.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m v \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/camkruse/projects/earthgenome/food-system-digital-twin/origins/origins.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     output_file\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39;49mdumps(feature) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/geo/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m skipkeys \u001b[39mand\u001b[39;00m ensure_ascii \u001b[39mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[39mand\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m indent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m separators \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort_keys \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_encoder\u001b[39m.\u001b[39;49mencode(obj)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/geo/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/geo/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/geo/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Decimal is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import json\n",
    "\n",
    "with open('../unsynced-data/ri-v2.geojson') as input_file, open('../unsynced-data/ri-nozero.geojson', 'w') as output_file:\n",
    "    # Create a JSON parser using ijson\n",
    "    parser = ijson.items(input_file, 'features.item')\n",
    "\n",
    "    # Iterate over the features in the input GeoJSON file\n",
    "    for feature in parser:\n",
    "        # Get the value of the \"crop_250\" property\n",
    "        v = feature['properties']\n",
    "\n",
    "        # If the value is not 0, write the feature to the output GeoJSON file\n",
    "        if v != 0:\n",
    "            output_file.write(json.dumps(feature) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
